{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with Generative Models: A Comparative Study of RAG and Traditional Retrieval Techniques\n",
    "\n",
    "#### Sarbpreet Ghotra & Sameer Ladha\n",
    "\n",
    "#### Emails: sarbpreet.ghotra@torontomu.ca & sameer.ladha@torontomu.ca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "#### Problem Description:\n",
    "The development of information retrieval systems has played a crucial role in managing the rapid expansion of online data.Traditional retrieval techniques have their strengths but often falter when dealing with the complexities of human language and nuanced user queries. The advent of Generative Retrieval methods, particularly the Retrieval-Augmented Generation (RAG) model, represents a significant leap forward in this domain. Drawing inspiration from the \"Learning to Tokenize for Generative Retrieval\" study, our approach adopts similar cutting-edge tokenization techniques. However, due to hardware limitations, we encountered challenges in deploying the full GenRet Model, which restricted our ability to fully explore its potential benefits. This project aims to evaluate the efficacy of RAG against standard retrieval methods like dense passage retrieval and sparse retrieval, as well as other Large Language Models (LLMs), by proposing an innovative method that integrates the reliability of traditional databases with the deep understanding of LLMs.\n",
    "\n",
    "#### Context of the Problem:\n",
    "Effectively responding to user queries, which can often be complex or vague, remains a significant challenge. Traditional retrieval systems, which typically rely on keyword searches or Boolean queries, frequently overlook the actual intent behind a query, leading to responses that are either irrelevant or incomplete (Gautier & Grave, 2022). With the continuous expansion of digital data, there's a pressing need for retrieval systems that not only understand the semantic subtleties of queries but also provide precise, comprehensive, and accurate answers.\n",
    "The challenge of navigating the immense landscape of online information and providing accurate, contextually relevant responses is substantial for retrieval systems (Weiwei et. al., 2023). Traditional methods often fall short in deciphering the nuanced meanings of user queries, resulting in off-target outcomes (Weiwei et. al., 2023).\n",
    "\n",
    "RAG introduces a paradigm shift by leveraging external knowledge to augment LLMs, with the source of retrieval and the granularity of retrieved units significantly influencing the outcome (Gao et. al., 2024). By incorporating external knowledge, RAG addresses the issue of generating factually incorrect information, marking its adoption as a cornerstone in advancing chatbots and enhancing LLMs for practical use (Gao et. al., 2024). The subsequent arrival of ChatGPT marked a pivotal moment, showcasing LLMs' remarkable ability for in-context learning (ICL) (Gao et. al., 2024). The focus of RAG research then expanded to provide LLMs with superior information for tackling more complex, knowledge-intensive tasks at the inference stage, propelling rapid advancements in the field (Gao et. al., 2024). As the research evolved, improvements in RAG began to encompass not just the inference stage but also the integration of LLM fine-tuning techniques (Gao et. al., 2024).\n",
    "\n",
    "#### Limitation About other Approaches:\n",
    "##### Sparse Retrieval Techniques\n",
    "- **TF-IDF and similar techniques:** Rank documents based on the presence or absence of specific terms in relation to a search query (Weiwei et. al., 2023).\n",
    "  - **Vocabulary mismatch:** Struggles with synonyms and words with multiple meanings, leading to overlooking relevant documents or retrieving unrelated ones (Weiwei et. al., 2023).\n",
    "  - **Lack of contextual understanding:** Ineffective for complex search queries due to inability to grasp the context in which terms are used (Weiwei et. al., 2023).\n",
    "  - **Static document and query representations:** Does not evolve to incorporate new information or shifts in language usage over time (Weiwei et. al., 2023).\n",
    "##### Dense Retrieval Approaches\n",
    "- **BERT model embeddings:** Use semantic similarities by representing queries and documents within a dense vector space (Weiwei et. al., 2023).\n",
    "  - **High computational demands:** Substantial computational power needed, especially for large datasets (Weiwei et. al., 2023).\n",
    "  - **Contextual ambiguities:** Difficulty interpreting ambiguous terms and complex queries requiring broader world knowledge (Weiwei et. al., 2023).\n",
    "  - **Dependence on data diversity and quality:** Success heavily reliant on the training data's diversity and quality (Weiwei et. al., 2023).\n",
    "  - **Fixed search procedure:** Challenges with end-to-end optimization due to a fixed index-retrieval pipeline (MIPS) and reliance on contrastive learning for training (Weiwei et. al., 2023).\n",
    "##### Limitations of Conventional Large Language Models (LLMs)\n",
    "- **Early versions of GPT or BERT:** Advanced natural language understanding but limited in direct retrieval tasks (Gao et. al., 2024).\n",
    "  - **Generative inaccuracies:** May produce fluent but factually incorrect or irrelevant responses, not linked to verified information sources (Gao et. al., 2024).\n",
    "  - **Generic responses:** Often provide broad, non-specific responses, unsatisfying for users needing detailed information (Gao et. al., 2024).\n",
    "  - **Resource intensity:** Significant computational resources required for real-time retrieval (Gao et. al., 2024).\n",
    "  - **Factual accuracy and relevance:** Despite improvements like GPT-3, challenges remain in ensuring outputs' accuracy and relevance (Gao et. al., 2024).\n",
    "\n",
    "\n",
    "\n",
    "#### Solution:\n",
    "The objective of our project is to delve into the capabilities and performance of advanced text retrieval and generation models. We aim to conduct a comprehensive comparison and application of Dense Passage Retrieval (DPR) models and the Pyserini toolkit, in conjunction with generative models and the Retrieval-Augmented Generation (RAG) model. This endeavor is designed to illuminate the strengths, limitations, and suitable applications of each model, providing a holistic understanding of their roles in contemporary information retrieval (IR) and natural language processing (NLP) tasks. The methodology encompasses detailed model descriptions, setup procedures, data preparation strategies, implementation steps, evaluation metrics, and a thorough comparative analysis.\n",
    "\n",
    "The adoption of DPR models in our project is motivated by their advanced approach to document retrieval, which significantly deviates from traditional sparse retrieval methods. Traditional methods, such as those based on keyword matching, often fall short in understanding the nuanced semantic relationships between words in queries and documents. DPR addresses this limitation by employing neural network architectures to generate dense vector embeddings for queries and documents. By leveraging separate BERT-based encoders, DPR can understand and capture the semantic essence of texts, facilitating a more accurate and relevant retrieval of documents. This capability is particularly crucial in handling complex queries that require a deep understanding of context and content, beyond mere keyword presence.\n",
    "\n",
    "Pyserini, on the other hand, serves as a versatile tool in our project, bridging the gap between traditional IR models and modern neural approaches. Its foundation on the Lucene search engine provides robust and efficient indexing and retrieval functionalities. By incorporating both sparse and dense retrieval mechanisms, Pyserini allows us to leverage the strengths of BM25 for keyword-focused searches while also enabling the integration of neural models like DPR for semantically rich queries. This dual capability makes Pyserini an invaluable asset for conducting scalable and comprehensive text retrieval operations across a wide range of datasets and query types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "| Reference                 | Explanation                                                                                                   | Dataset/Input | Weakness/Future Improvement                                       |\n",
    "| ------------------------- | ------------------------------------------------------------------------------------------------------------- | ------------- | ----------------------------------------------------------------- |\n",
    "| Khodabakhsh & Bagheri     | Developed a framework for ranking documents and predicting list quality for a specific query. | MS MARCO      | Method applies post-retrieval rather than pre-retrieval. |\n",
    "| Gao et al.                | Conducted a survey on various RAG techniques to enhance understanding of current research.                                         | 50+ Datasets (MS MARCO, NQ320K, SQuAD)| Need to refine evaluation methodologies to match its evolution.|\n",
    "| Gautier & Grave           | Used attention scores from a reader model, which solved tasks based on retrieved documents, to generate synthetic labels for the retriever.|TriviaQA & NQ| Did not reinitialize the retriever between iterations.|\n",
    "| Gustavo & Hauff           | Analyzed both supervised and unsupervised, dense and sparse retrieval models for the underexplored area of full-rank dialogue response retrieval. |TREC-DL| Could further optimize the bi-encoder dense retrieval model for enhanced performance.|\n",
    "| Weiwei et al.             | Proposed GENRET, a novel document tokenization method that encodes complete document semantics into docids.|MS MARCO, BEIR, NQ320K| Implementation requires significant computational resources.|\n",
    "| Karpukhin et al.          | Demonstrated that dense representations alone can suffice for retrieval.   |NQ, TriviaQA, WQ, TREC, SQuAD| Qualitative differences in retrieved passages when comparing retrievers.|\n",
    "| Lin et al.                | Introduced a user-friendly Python toolkit for replicable IR research, offering effective first-stage retrieval in multistage ranking architectures.|MS MARCO| Aims to include more dense retrieval techniques based on learned representations.|\n",
    "| Lewis et al.              | Introduced RAG models combining a pre-trained seq2seq model (parametric memory) with a dense vector index of Wikipedia (non-parametric memory).|Custom Wikipedia Dump, MS MARCO, Jeopardy Question Generation| Custom data source may not be fully accurate, potentially causing hallucinations similar to GPT models.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "## Our Solution\n",
    "\n",
    "Our solution utilizes the **Retrieval-Augmented Generation (RAG)** model to address the observed shortcomings in both traditional retrieval methods and Large Language Models (LLMs). Our research aimed to compare the effectiveness of various document retrieval models:\n",
    "\n",
    "- The Retrieval-Augmented Generation (RAG) model\n",
    "- The BERT model\n",
    "- The Sparse Model using Pyserini\n",
    "- The Dense Model using Dense Passage Retrieval (DPR)\n",
    "\n",
    "This comparison allowed us to evaluate and analyze each model's capacity to retrieve relevant documents based on specific queries, aligning with our overall objective of enhancing information retrieval systems.\n",
    "\n",
    "### Practical Applications and Performance\n",
    "\n",
    "To better understand the practical applications and performance of each document retrieval model, we conducted example searches using specific queries. These searches were designed to illustrate how each model processes and responds to real-world queries, providing a direct view of their operational effectiveness and differences. For all the models listed, we utilized the **Microsoft Research WikiQA Corpus**, a publicly available dataset designed specifically for open-domain question answering research.\n",
    "\n",
    "This dataset consists of question and sentence pairs derived from Bing query logs, ensuring a close reflection of real user inquiries. Each question is associated with a Wikipedia page, targeting the summary section which typically contains the most vital information about the topic. To build our candidate answer set, we selected sentences from these summary sections. The dataset encompasses 3,047 questions and 29,258 sentences, out of which 1,473 sentences are annotated as correct answers to the corresponding questions. This rich dataset facilitated a rigorous evaluation of our models, testing their capability to understand and retrieve accurate answers from a broad knowledge base.\n",
    "\n",
    "![Figure 1 taken from (Gao et. al., 2024). ](<Screenshot 2024-04-19 at 08.45.58.png>)\n",
    "*RAG Workflow*\n",
    "\n",
    "### User Input and Information Retrieval\n",
    "\n",
    "A user submits a search request which RAG then transforms into a vector form. The vector of the user's search is matched against pre-existing vectors in the database using a relevancy search algorithm. This algorithm locates the entries (documents) in the knowledge base that are most semantically similar to the user's search, ensuring precise and relevant retrieval of information.\n",
    "\n",
    "### Extracting Relevant Information\n",
    "\n",
    "Following the relevancy search, the documents or segments that are most pertinent to the search results are gathered from external data sources. This step ensures that only the most relevant information is retrieved to be used in the subsequent processes.\n",
    "\n",
    "### Prompt Augmentation\n",
    "\n",
    "RAG merges the original user query with the newly acquired pertinent data. By utilizing prompt engineering techniques, the system enhances the context and directives provided to the LLM. This enhanced prompt equips the LLM with a deeper understanding of what the user is seeking, improving the system's responsiveness and accuracy.\n",
    "\n",
    "### Enhanced Response Generation\n",
    "\n",
    "With the enriched prompt, the LLM has access to both the initial user query and the additional relevant data. This allows the LLM to produce a response that is both more precise and informative, enhancing the overall user experience by providing detailed and accurate answers.\n",
    "\n",
    "### Maintaining Data Freshness\n",
    "\n",
    "To ensure that the retrieved information remains up-to-date, it is crucial to regularly update the external data sources and their associated vector representations. Depending on the application's specific requirements, this can be achieved through automated processes in real-time or scheduled batch updates, maintaining the system's effectiveness and reliability.\n",
    "\n",
    "### The Sparse Model\n",
    "\n",
    "Understanding the performance of the Sparse Model helps delineate the contrast between traditional retrieval methods and more contemporary approaches that incorporate neural networks and vector space modeling. The implementation process was executed seamlessly through Pyserini, leveraging Lucene's search engine capabilities. Initially, our dataset underwent preprocessing to ensure compatibility with the JSONL format required for indexing. Subsequently, Lucene's indexing functionalities facilitated the creation of an efficient search index, optimizing retrieval performance. To evaluate the Sparse Model's efficacy, we employed standard information retrieval metrics, providing a comprehensive assessment on the test dataset. These metrics provided a quantitative assessment of how well the Sparse Model managed to retrieve relevant documents. Precision and recall offered insights into the accuracy and completeness of the retrieval process, while F1-score provided a harmonic mean of precision and recall. MRR gave us an understanding of the ranking effectiveness of the retrieval system.\n",
    "\n",
    "### The Dense Model\n",
    "\n",
    "This model leverages neural networks to transform text into dense vector spaces, enabling more nuanced and semantically rich retrieval capabilities compared to traditional sparse techniques. This involved configuring Facebook's DPR framework, which encodes questions and passages into dense vectors independently. Custom dataset handling was crucial in preparing the data for this model. We utilized a custom dataset class, WikiQADataset, to manage the data efficiently. This class was responsible for loading and organizing the data from a file, ensuring that each entry was correctly formatted with questions, contexts, and labels. By encoding questions and contexts into high-dimensional vectors using pretrained DPR models, we enabled efficient similarity calculation through cosine similarity metrics. These metrics, combined with standard retrieval evaluation measures, allowed us to quantify the Dense Model's retrieval effectiveness accurately. The model's effectiveness was quantified using precision, recall, F1-score, and Mean Reciprocal Rank (MRR).\n",
    "\n",
    "### BERT Model\n",
    "\n",
    "The BERT model in our study highlights its advanced capabilities in processing and classifying large-scale text data for retrieval purposes. Employing the BERT model for sequence classification required a meticulous approach. Initially, data preparation involved pairing questions with answer candidates and labeling them for binary classification. Leveraging a pretrained BERT model tailored for sequence classification tasks, we configured our model to suit our specific needs. We utilized the bert-base-uncased model and its corresponding tokenizer. To handle large datasets efficiently, data was processed in batches. This approach ensures that the model does not exceed memory limits and allows for parallel processing of data. Through a comprehensive evaluation process, including retrieval metrics and accuracy analysis, we gained insights into the BERT model's performance, ensuring a thorough understanding of its effectiveness in our context.\n",
    "\n",
    "### Retrieval-Augmented Generation (RAG) Model\n",
    "\n",
    "In the context of our research on document retrieval systems, the method employed for the RAG model involved a series of detailed steps that integrate both retrieval and generative components to answer complex queries effectively. This model leverages the strengths of both dense vector retrievals for nuanced understanding of query context and generative models for producing informative, naturally phrased responses.\n",
    "\n",
    "### GenRet Model\n",
    "\n",
    "In our exploration of the GenRet model, as detailed in the \"Learning to Tokenize for Generative Retrieval'' research, we encountered significant hardware demands that impeded our ability to fully implement the system. Despite efforts to mitigate these challenges through adjustments such as reducing the number of epochs and scaling down the dataset size, we consistently faced errors that prevented successful output generation. To enhance our computational capabilities, we invested in additional resources, including Google Colab Pro and Paperspace Pro, and utilized high-performance GPUs like the A4000 and P4000, which incurred hourly charges. This experience highlighted a critical aspect of working with large-scale models and extensive datasets: the substantial hardware requirements and financial implications that can pose significant barriers to research and development in this advanced area of study. On the contrary the RAG model's methodology was designed to showcase how advanced retrieval techniques combined with state-of-the-art language models can significantly improve the relevance and quality of information retrieval systems.\n",
    "\n",
    "![Figure 2 taken from Weiwei et. al., 2023.](Information-Retrieval-with-Generative-Models/figs/Gao et. al., 2024.png)\n",
    "\n",
    "Document retrieval includes various models, including two distinct approaches. The first method, known as Dense retrieval, involves encoding both queries and documents into dense vectors and subsequently retrieving documents using the Most Important Points Selection (MIPS) technique (Weiwei et. al., 2023). Conversely, Generative retrieval operates by tokenizing documents into document identifiers (docids) and autonomously generating these docids as retrieval outcomes through an autoregressive process (Weiwei et. al., 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyserini transformers langchain\n",
    "\n",
    "#For other machines, such as an (M-Series Mac or Linux), use the following link to install Pyserini: \n",
    "#https://github.com/castorini/pyserini/blob/master/docs/installation.md\n",
    "\n",
    "#You also might need Java11 installed on your machine. \n",
    "#brew install openjdk@11\n",
    "\n",
    "#For the RAG implementation using Langcahin, you will need an OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameerladha/miniconda3/envs/datamlenv/lib/python3.11/site-packages/transformers/utils/generic.py:462: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/sameerladha/miniconda3/envs/datamlenv/lib/python3.11/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/sameerladha/miniconda3/envs/datamlenv/lib/python3.11/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/sameerladha/miniconda3/envs/datamlenv/lib/python3.11/site-packages/transformers/utils/generic.py:319: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import getpass\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer, DPRQuestionEncoder, DPRContextEncoder\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the dataset\n",
    "train_dataset = \"WikiQA-train.tsv\"\n",
    "dev_dataset = \"WikiQA-dev.tsv\"\n",
    "test_dataset = \"WikiQA-test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the question context pairs youd like to use\n",
    "#We utilized ChatGPT to generate the questions and the short and long contexts\n",
    "\n",
    "# question = \"What causes the seasons to change?\"\n",
    "# contextslong = [\n",
    "#     \"The change of seasons is caused by the tilt of the Earth's rotational axis away or toward the Sun as it travels through its year-long path around the Sun.\",\n",
    "#     \"Seasons change due to the tilt of the Earth's axis, affecting how much sunlight different parts of Earth receive.\",\n",
    "#     \"The Earth's tilt and its orbit around the Sun lead to changes in the intensity and duration of sunlight received, causing seasons.\"\n",
    "# ]\n",
    "# contextsshort = [\n",
    "#     \"Seasons change because of Earth's tilt.\",\n",
    "#     \"The tilt affects sunlight received.\",\n",
    "#     \"Earth's orbit and tilt cause seasons.\"\n",
    "# ]\n",
    "\n",
    "# question = \"Who was the first person to walk on the Moon?\"\n",
    "# contextslong = [\n",
    "#     \"The first person to walk on the Moon was Neil Armstrong on July 20, 1969, during the Apollo 11 mission.\",\n",
    "#     \"Neil Armstrong, an American astronaut, made history by being the first human to step on the lunar surface.\",\n",
    "#     \"Apollo 11 was the spaceflight that landed the first two people on the Moon. Neil Armstrong was the mission commander.\"\n",
    "# ]\n",
    "# contextsshort = [\n",
    "#     \"Neil Armstrong walked on the Moon.\",\n",
    "#     \"The first moonwalk was by an American.\",\n",
    "#     \"Apollo 11 landed the first humans on the Moon.\"\n",
    "# ]\n",
    "\n",
    "question = \"What is the main theme of 'To Kill a Mockingbird'?\"\n",
    "contextslong = [\n",
    "    \"'To Kill a Mockingbird' explores themes of racial injustice and the destruction of innocence.\",\n",
    "    \"Harper Lee's novel is renowned for its warmth and humor, despite dealing with serious issues of rape and racial inequality.\",\n",
    "    \"The main themes of 'To Kill a Mockingbird' include racial injustice, moral growth, and the exploration of gender roles in American society.\"\n",
    "]\n",
    "contextsshort = [\n",
    "    \"The theme includes racial injustice.\",\n",
    "    \"It deals with serious societal issues.\",\n",
    "    \"Themes of moral growth and gender roles.\"\n",
    "]\n",
    "\n",
    "#question = \"What causes rain?\"\n",
    "#contextslong = [\n",
    "\n",
    "# \"Rain is liquid water in the form of droplets that have condensed from atmospheric water vapor and then become heavy enough to fall under gravity.\",\n",
    "# \"Rain is a type of precipitation, a product of the condensation of atmospheric water vapor that is deposited on the Earth's surface.\",\n",
    "# \"The primary cause of rain production is moisture moving along three-dimensional zones of temperature and moisture contrasts known as weather fronts.\"\n",
    "#]\n",
    "\n",
    "# contextsshort = [\n",
    "# \"Rain is liquid water in the form of droplets.\",\n",
    "# \"Rain is a type of precipitation.\",\n",
    "# \"Rain forms from moisture.\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sameerladha/Documents/School/Masters of Science - Data Science and Analytics/S2/DS8008 - NLP/Final Project/FF/WikiQACorpus\n",
      "2024-04-19 12:53:14,167 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:204) - Setting log level to INFO\n",
      "2024-04-19 12:53:14,168 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:207) - ============ Loading Index Configuration ============\n",
      "2024-04-19 12:53:14,168 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:208) - AbstractIndexer settings:\n",
      "2024-04-19 12:53:14,168 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:209) -  + DocumentCollection path: .\n",
      "2024-04-19 12:53:14,168 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:210) -  + CollectionClass: JsonCollection\n",
      "2024-04-19 12:53:14,168 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:211) -  + Index path: indexes/wikiqa\n",
      "2024-04-19 12:53:14,169 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:212) -  + Threads: 4\n",
      "2024-04-19 12:53:14,169 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:213) -  + Optimize (merge segments)? false\n",
      "Apr 19, 2024 12:53:14 P.M. org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "2024-04-19 12:53:14,187 INFO  [main] index.IndexCollection (IndexCollection.java:246) - Using DefaultEnglishAnalyzer\n",
      "2024-04-19 12:53:14,187 INFO  [main] index.IndexCollection (IndexCollection.java:247) - Stemmer: porter\n",
      "2024-04-19 12:53:14,187 INFO  [main] index.IndexCollection (IndexCollection.java:248) - Keep stopwords? false\n",
      "2024-04-19 12:53:14,188 INFO  [main] index.IndexCollection (IndexCollection.java:249) - Stopwords file: null\n",
      "2024-04-19 12:53:14,258 INFO  [main] index.IndexCollection (IndexCollection.java:197) - IndexCollection settings:\n",
      "2024-04-19 12:53:14,258 INFO  [main] index.IndexCollection (IndexCollection.java:198) -  + Generator: DefaultLuceneDocumentGenerator\n",
      "2024-04-19 12:53:14,259 INFO  [main] index.IndexCollection (IndexCollection.java:199) -  + Language: en\n",
      "2024-04-19 12:53:14,259 INFO  [main] index.IndexCollection (IndexCollection.java:200) -  + Stemmer: porter\n",
      "2024-04-19 12:53:14,259 INFO  [main] index.IndexCollection (IndexCollection.java:201) -  + Keep stopwords? false\n",
      "2024-04-19 12:53:14,259 INFO  [main] index.IndexCollection (IndexCollection.java:202) -  + Stopwords: null\n",
      "2024-04-19 12:53:14,259 INFO  [main] index.IndexCollection (IndexCollection.java:203) -  + Store positions? true\n",
      "2024-04-19 12:53:14,259 INFO  [main] index.IndexCollection (IndexCollection.java:204) -  + Store docvectors? true\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.IndexCollection (IndexCollection.java:205) -  + Store document \"contents\" field? true\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.IndexCollection (IndexCollection.java:206) -  + Store document \"raw\" field? true\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.IndexCollection (IndexCollection.java:207) -  + Additional fields to index: []\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.IndexCollection (IndexCollection.java:208) -  + Whitelist: null\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.IndexCollection (IndexCollection.java:209) -  + Pretokenized?: false\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.IndexCollection (IndexCollection.java:210) -  + Codec: Lucene99\n",
      "2024-04-19 12:53:14,260 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:237) - ============ Indexing Collection ============\n",
      "2024-04-19 12:53:14,263 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:246) - Thread pool with 4 threads initialized.\n",
      "2024-04-19 12:53:14,263 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:247) - 1 file found in .\n",
      "2024-04-19 12:53:14,263 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:248) - Starting to index...\n",
      "2024-04-19 12:53:15,366 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:306) - Indexing Complete! 29,258 documents indexed\n",
      "2024-04-19 12:53:15,366 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:307) - ============ Final Counter Values ============\n",
      "2024-04-19 12:53:15,367 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:308) - indexed:           29,258\n",
      "2024-04-19 12:53:15,367 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:309) - unindexable:            0\n",
      "2024-04-19 12:53:15,367 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:310) - empty:                  0\n",
      "2024-04-19 12:53:15,367 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:311) - skipped:                0\n",
      "2024-04-19 12:53:15,367 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:312) - errors:                 0\n",
      "2024-04-19 12:53:15,370 INFO  [main] index.AbstractIndexer (AbstractIndexer.java:315) - Total 29,258 documents indexed in 00:00:01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Apr 19, 2024 12:53:15 P.M. org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "Metric       | Score\n",
      "------------ + ------------\n",
      "Precision    | 0.1254\n",
      "Recall       | 0.4949\n",
      "F1-Score     | 0.2001\n",
      "MRR          | 0.0019\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Sparse Retrieval using Pyserini\n",
    "\n",
    "# Function to preprocess text files into a single JSONL file for indexing\n",
    "def preprocess_to_jsonl(input_files, output_file):\n",
    "    with open(output_file, 'w') as out:\n",
    "        for input_file in input_files:\n",
    "            with open(input_file, 'r', encoding='utf-8') as f:\n",
    "                next(f)  # Skip the header line of the file\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) < 6: continue\n",
    "                    doc_id, text = parts[4], parts[5]\n",
    "                    # Create a JSON object with the document ID and text\n",
    "                    json_object = json.dumps({'id': doc_id, 'contents': text})\n",
    "                    out.write(json_object + '\\n')\n",
    "\n",
    "# Preprocess and index the dataset\n",
    "preprocess_to_jsonl([train_dataset, dev_dataset, test_dataset], \"wikiqa.jsonl\")\n",
    "\n",
    "%cd Local/path/to/your/directory/containing/WikiQACorpus\n",
    "!python -m pyserini.index.lucene -collection JsonCollection -generator DefaultLuceneDocumentGenerator -threads 4 -input . -index indexes/wikiqa -storePositions -storeDocvectors -storeRaw -storeContents\n",
    "\n",
    "# Function to evaluate the sparse retrieval model\n",
    "def evaluate_sparse(test_dataset_path, searcher, top_k=5):\n",
    "    all_labels, all_preds = [], []\n",
    "    \n",
    "    with open(test_dataset_path, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # Skip the header line of the file\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 6: continue\n",
    "            question, doc_id, label = parts[1], parts[4], int(parts[6])\n",
    "            \n",
    "            # Search the index for the question and retrieve the top k results\n",
    "            hits = searcher.search(question, k=top_k)\n",
    "            retrieved_doc_ids = [hit.docid for hit in hits]\n",
    "            # Determine if the true document ID is among the retrieved document IDs\n",
    "            pred_label = 1 if doc_id in retrieved_doc_ids else 0\n",
    "            \n",
    "            # Append the true label and predicted label to their respective lists\n",
    "            all_labels.append(label)\n",
    "            all_preds.append(pred_label)\n",
    "    \n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    # Calculate Mean Reciprocal Rank (MRR)\n",
    "    mrr = np.mean([1.0 / (rank + 1) for rank, label in enumerate(all_preds) if label == 1])\n",
    "    \n",
    "    return precision, recall, f1, mrr\n",
    "\n",
    "# Initialize the searcher with the path to the index\n",
    "searcher = LuceneSearcher('/WikiQACorpus/indexes/wikiqa')\n",
    "\n",
    "# Evaluate the sparse retrieval model using the test dataset and the searcher\n",
    "precision, recall, f1, mrr = evaluate_sparse(test_dataset, searcher)\n",
    "print ()\n",
    "print ('---------------------------------')\n",
    "print(f\"{'Metric':<12} | {'Score'}\")\n",
    "print(f\"{'-'*12} + {'-'*12}\")\n",
    "print(f\"{'Precision':<12} | {precision:.4f}\")\n",
    "print(f\"{'Recall':<12} | {recall:.4f}\")\n",
    "print(f\"{'F1-Score':<12} | {f1:.4f}\")\n",
    "print(f\"{'MRR':<12} | {mrr:.4f}\")\n",
    "print ('---------------------------------')\n",
    "print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will search for the question: What is the main theme of 'To Kill a Mockingbird'? :\n",
      "\n",
      " 1 (score: 6.57380): D17-8 - According to Rowling, the main theme is death.\n",
      " 2 (score: 5.96600): D1914-4 - The main theme of the film is taken from the tune \"The Gael\" by Scottish singer-songwriter Dougie MacLean .\n",
      " 3 (score: 5.75170): D1057-6 - The main characters can be killed, and certain actions may lead to different scenes and endings.\n",
      " 4 (score: 5.64410): D116-2 - His main role was to interpret the will of the gods by studying the flight of birds : whether they are flying in groups or alone, what noises they make as they fly, direction of flight and what kind of birds they are.\n",
      " 5 (score: 5.52790): D1061-4 - Domestic policy and the economy eventually emerged as the main themes in the last few months of the election campaign after the onset of the worst recession since the 1930s .\n",
      " 6 (score: 5.52790): D1061-4 - Domestic policy and the economy eventually emerged as the main themes in the last few months of the election campaign after the onset of the worst recession since the 1930s .\n",
      " 7 (score: 5.52790): D1061-4 - Domestic policy and the economy eventually emerged as the main themes in the last few months of the election campaign after the onset of the worst recession since the 1930s .\n",
      " 8 (score: 5.37910): D1566-5 - Within those platoons, various specific positions exist depending on what the player's main job is.\n",
      " 9 (score: 5.37910): D1566-5 - Within those platoons, various specific positions exist depending on what the player's main job is.\n",
      "10 (score: 4.74430): D1132-9 - The episodes use three pieces of theme music : one opening theme and two closing themes.\n"
     ]
    }
   ],
   "source": [
    "#Perform an example search using the Sparse Retrieval Model\n",
    "\n",
    "# Function to perform a search and print the results\n",
    "def search(query, searcher, top_k=10):\n",
    "    # Search the index for the query and retrieve the top k results\n",
    "    hits = searcher.search(query, k=top_k)\n",
    "    for i, hit in enumerate(hits, start=1):\n",
    "        doc = searcher.doc(hit.docid)\n",
    "        content = doc.lucene_document().get(\"contents\") \n",
    "        print(f'{i:2} (score: {hit.score:.5f}): {hit.docid} - {content}')\n",
    "\n",
    "print (\"We will search for the question:\",question,\":\\n\")\n",
    "search(question,searcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1113ac17fb40a7a4b4db5e8e29b124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric         Score\n",
      "--------------------\n",
      "Precision     0.0612\n",
      "Recall        0.9643\n",
      "F1-Score      0.1151\n",
      "MRR           0.5707\n"
     ]
    }
   ],
   "source": [
    "#Dense Retrival using Dense Passage Retrieval (DPR) model\n",
    "\n",
    "# Initialize tokenizers for encoding the questions and contexts\n",
    "question_encoder_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "# Initialize the models for encoding questions and contexts into embeddings\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "context_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "\n",
    "class WikiQADataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.data = []\n",
    "        # Load data from a file\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            next(file)  # skip header line\n",
    "            for line in file:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 7: continue  # skip incomplete lines\n",
    "                # Append question, context, and label to the dataset\n",
    "                self.data.append({\n",
    "                    'question': parts[1],\n",
    "                    'context': parts[5],\n",
    "                    'label': int(parts[6])\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of items in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a specific item from the dataset by index\n",
    "        return self.data[idx]\n",
    "\n",
    "# Function to encode a pair of question and context into embeddings\n",
    "def encode_questions_contexts(question, contexts):\n",
    "    question_encoding = question_encoder_tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    context_encoding = context_encoder_tokenizer(contexts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        question_emb = question_encoder(**question_encoding).pooler_output\n",
    "        context_embs = context_encoder(**context_encoding).pooler_output.squeeze(1)  \n",
    "    return question_emb, context_embs\n",
    "\n",
    "# Function to evaluate the model on the test dataset\n",
    "def evaluate_dpr(test_dataset_path):\n",
    "    test_data = WikiQADataset(test_dataset_path)\n",
    "    test_loader = DataLoader(test_data, batch_size=4)\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for item in tqdm(test_loader):\n",
    "        question = item['question'][0]  \n",
    "        context = item['context'][0]\n",
    "        label = item['label'][0].item()  \n",
    "\n",
    "        question_emb, context_emb = encode_questions_contexts([question], [context])\n",
    "\n",
    "        # Calculate cosine similarity between question and context embeddings\n",
    "        cos_sim = torch.nn.functional.cosine_similarity(question_emb, context_emb).item()\n",
    "        # Predict relevance based on cosine similarity (simple thresholding)\n",
    "        pred = 1 if cos_sim > 0.5 else 0\n",
    "        # Collect all true labels and predictions\n",
    "        all_labels.append(label)\n",
    "        all_preds.append(pred)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    # Calculate Mean Reciprocal Rank (MRR)\n",
    "    mrr = np.mean([1.0 / (rank + 1) for rank in all_preds])\n",
    "\n",
    "    # Return the evaluation metrics\n",
    "    return precision, recall, f1, mrr\n",
    "\n",
    "precision, recall, f1, mrr = evaluate_dpr(test_dataset)\n",
    "print(f\"{'Metric':<10}{'Score':>10}\")\n",
    "print(f\"{'-'*20}\")\n",
    "print(f\"{'Precision':<10}{precision:>10.4f}\")\n",
    "print(f\"{'Recall':<10}{recall:>10.4f}\")\n",
    "print(f\"{'F1-Score':<10}{f1:>10.4f}\")\n",
    "print(f\"{'MRR':<10}{mrr:>10.4f}\")\n",
    "\n",
    "#using the question 'What causes rain' and the contexts provided above\n",
    "#With Batch Size 1 Precision: 0.0531, Recall: 0.9693, F1-Score: 0.1007, MRR: 0.5663\n",
    "#With Batch Size 4 Precision: 0.0612 Recall: 0.9643 F1-Score: 0.1151 MRR: 0.5707\n",
    "#With Batch Size 8 Precision: 0.0602, Recall: 0.9524, F1-Score: 0.1133, MRR: 0.5694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will search for the question: What is the main theme of 'To Kill a Mockingbird'? :\n",
      "\n",
      "The contexts are:\n",
      "\n",
      "[\"'To Kill a Mockingbird' explores themes of racial injustice and the destruction of innocence.\", \"Harper Lee's novel is renowned for its warmth and humor, despite dealing with serious issues of rape and racial inequality.\", \"The main themes of 'To Kill a Mockingbird' include racial injustice, moral growth, and the exploration of gender roles in American society.\"]\n",
      "\n",
      "Top contexts and their scores:\n",
      "Score: 0.7368, Context: 'To Kill a Mockingbird' explores themes of racial injustice and the destruction of innocence.\n",
      "Score: 0.5559, Context: The main themes of 'To Kill a Mockingbird' include racial injustice, moral growth, and the exploration of gender roles in American society.\n",
      "Score: 0.7340, Context: Harper Lee's novel is renowned for its warmth and humor, despite dealing with serious issues of rape and racial inequality.\n"
     ]
    }
   ],
   "source": [
    "#Perform an example search using the DPR model\n",
    "\n",
    "def search_dpr(question, contexts, top_k=5):\n",
    "    question_emb, context_embs = encode_questions_contexts(question, contexts)\n",
    "    # Calculate cosine similarity between the question embedding and each context embedding\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(question_emb.repeat(context_embs.size(0), 1), context_embs)\n",
    "    top_k_values, top_k_indices = torch.topk(cos_sim, k=top_k)\n",
    "    top_contexts = [(contexts[i], top_k_values[i].item()) for i in top_k_indices]\n",
    "    return top_contexts\n",
    "\n",
    "# Perform an example search\n",
    "print (\"We will search for the question:\", question,\":\\n\")\n",
    "print (\"The contexts are:\\n\")\n",
    "print (contextslong)\n",
    "print ()\n",
    "\n",
    "top_k = len(contextslong)\n",
    "top_contexts = search_dpr(question, contextslong, top_k=top_k)\n",
    "print(\"Top contexts and their scores:\")\n",
    "for context, score in top_contexts:\n",
    "    print(f\"Score: {score:.4f}, Context: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will search for the question: What is the main theme of 'To Kill a Mockingbird'? :\n",
      "\n",
      "The contexts are:\n",
      "\n",
      "['The theme includes racial injustice.', 'It deals with serious societal issues.', 'Themes of moral growth and gender roles.']\n",
      "\n",
      "Top contexts and their scores:\n",
      "Score: 0.5817, Context: The theme includes racial injustice.\n",
      "Score: 0.5601, Context: Themes of moral growth and gender roles.\n",
      "Score: 0.5615, Context: It deals with serious societal issues.\n"
     ]
    }
   ],
   "source": [
    "# Perform an example search\n",
    "print (\"We will search for the question:\", question,\":\\n\")\n",
    "print (\"The contexts are:\\n\")\n",
    "print (contextsshort)\n",
    "print ()\n",
    "\n",
    "top_k = len(contextsshort)\n",
    "top_contexts = search_dpr(question, contextsshort, top_k=top_k)\n",
    "print(\"Top contexts and their scores:\")\n",
    "for context, score in top_contexts:\n",
    "    print(f\"Score: {score:.4f}, Context: {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86551ca9dd024b1083ebec9ad3f626d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric         Score\n",
      "--------------------\n",
      "Accuracy      0.9556\n",
      "Recall        0.0988\n",
      "F1-Score      0.1716\n",
      "MRR           0.0004\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # For binary classification\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to prepare data for BERT\n",
    "def prepare_data(dataset_path):\n",
    "    question_sentence_pairs, labels = [], []\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "        next(file)  # Skip the header line\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 7: continue\n",
    "            question, sentence, label = parts[1], parts[5], int(parts[6])\n",
    "            question_sentence_pairs.append((question, sentence))\n",
    "            labels.append(label)\n",
    "    return question_sentence_pairs, labels\n",
    "\n",
    "# Function to evaluate using BERT\n",
    "def evaluate_bert(dataset_path, batch_size=16):\n",
    "    question_sentence_pairs, true_labels = prepare_data(dataset_path)\n",
    "    all_labels, all_preds = [], []\n",
    "    all_scores = []  # Collect scores for MRR calculation\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_batches = len(question_sentence_pairs) // batch_size + (1 if len(question_sentence_pairs) % batch_size != 0 else 0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_pairs, batch_labels in tqdm(zip(\n",
    "                (question_sentence_pairs[i*batch_size:(i+1)*batch_size] for i in range(total_batches)),\n",
    "                (true_labels[i*batch_size:(i+1)*batch_size] for i in range(total_batches))\n",
    "            ), total=total_batches, desc=\"Evaluating\"):\n",
    "            inputs = tokenizer(batch_pairs, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "            outputs = model(**inputs)\n",
    "            pred_labels = torch.argmax(outputs.logits, dim=1)\n",
    "            all_labels.extend(batch_labels)\n",
    "            all_preds.extend(pred_labels.tolist())\n",
    "            all_scores.extend(outputs.logits[:,1].detach().cpu().numpy())  # Assuming label 1 is the \"positive\" class\n",
    "\n",
    "    # Calculate MRR\n",
    "    ranks = []\n",
    "    for score, label in zip(all_scores, all_labels):\n",
    "        ordered_scores = np.argsort(-np.array(all_scores))  # Sort scores descending\n",
    "        rank = np.where(ordered_scores == label)[0][0] + 1\n",
    "        ranks.append(1 / rank)\n",
    "    mrr = np.mean(ranks)\n",
    "\n",
    "    # Calculate other metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "    return precision, recall, f1, accuracy, mrr\n",
    "\n",
    "# Evaluate the model using the test dataset\n",
    "metrics = evaluate_bert(test_dataset)\n",
    "accuracy = metrics[1]\n",
    "recall = metrics[2]\n",
    "f1 = metrics[3]\n",
    "mrr = metrics[4]\n",
    "print(f\"{'Metric':<10}{'Score':>10}\")\n",
    "print(f\"{'-'*20}\")\n",
    "print(f\"{'Accuracy':<10}{accuracy:>10.4f}\")\n",
    "print(f\"{'Recall':<10}{recall:>10.4f}\")\n",
    "print(f\"{'F1-Score':<10}{f1:>10.4f}\")\n",
    "print(f\"{'MRR':<10}{mrr:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will search for the question: What is the main theme of 'To Kill a Mockingbird'? :\n",
      "\n",
      " 1 (score: 0.20367): From quiet beginnings in the Shire , a Hobbit land not unlike the English countryside, the story ranges across north-west Middle-earth, following the course of the War of the Ring through the eyes of its characters, notably the hobbits Frodo Baggins , Samwise \"Sam\" Gamgee , Meriadoc \"Merry\" Brandybuck and Peregrin \"Pippin\" Took , but also the hobbits' chief allies and travelling companions: Aragorn , a Human Ranger ; Boromir , a man from Gondor ; Gimli , a Dwarf warrior; Legolas , an Elven prince; and Gandalf , a Wizard.\n",
      " 2 (score: 0.13070): For example, a three-note triad using C as a root would be C-E-G.\n",
      " 3 (score: 0.12939): In some traditions, baptism is also called christening, but for others the word \"christening\" is reserved for the baptism of infants .\n",
      " 4 (score: 0.12635): Wind power is the conversion of wind energy into a useful form of energy, such as using wind turbines to make electrical power , windmills for mechanical power, wind pumps for water pumping or drainage , or sails to propel ships.\n",
      " 5 (score: 0.12076): Phonograph records are generally described by their diameter in inches (12-inch, 10-inch, 7-inch, etc.), the rotational speed at which they are played (\"33 rpm \", \"78\", \"45\", etc.), their time capacity (\"long playing\"), their reproductive accuracy, or \" fidelity \" (\"high fidelity\", \"Orthophonic\", \"full-range\", etc.), and the number of channels of audio provided (\" mono \", \" stereo \", \" quadraphonic \", etc.).\n",
      " 6 (score: 0.11516): For example, in the Gregorian calendar (a common solar calendar), February in a leap year has 29 days instead of the usual 28, so the year lasts 366 days instead of the usual 365.\n",
      " 7 (score: 0.11334): However, there are uplands in the north (for example, the mountainous Lake District , Pennines , and Yorkshire Dales ) and in the south west (for example, Dartmoor and the Cotswolds ).\n",
      " 8 (score: 0.10932): Karma ( Sanskrit , also karman, Pāli : Kamma) means \"action\" or \"doing\"; whatever one does, says, or thinks is a karma.\n",
      " 9 (score: 0.10432): Similar words exist in other languages: Dom (German), Dóm (Hungarian & Slovakian), Dôme (French), Domkirke (Danish), Dómkirkja (Icelandic),Domo (Portuguese) Domkyrka (Swedish), Domkirke (Norwegian), Doms (Latvian), Toomkirik (Estonian), Tum (Polish), and Tuomiokirkko (Finnish).\n",
      "10 (score: 0.10317): Vaccines may be prophylactic (example: to prevent or ameliorate the effects of a future infection by any natural or \"wild\" pathogen ), or therapeutic (e.g. vaccines against cancer are also being investigated; see cancer vaccine ).\n"
     ]
    }
   ],
   "source": [
    "# Function to perform a search and print the results using BERT\n",
    "def search_bert(query, model, tokenizer, dataset_path, top_k=10):\n",
    "    question_sentence_pairs, _ = prepare_data(dataset_path)\n",
    "    scores = []\n",
    "\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for (_, sentence) in question_sentence_pairs:  # Changed to search the entire dataset\n",
    "            input_text = query + \" [SEP] \" + sentence  # Use BERT's recommended format for pair inputs\n",
    "            inputs = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "            output = model(**inputs)\n",
    "            score = output.logits[0][1].item()  # Probability of positive class (assumed relevance)\n",
    "            scores.append((score, sentence))\n",
    "\n",
    "    # Sort the results by score in descending order\n",
    "    sorted_results = sorted(scores, key=lambda x: x[0], reverse=True)[:top_k]\n",
    "    \n",
    "    # Print the top-k results\n",
    "    for i, (score, sentence) in enumerate(sorted_results, start=1):\n",
    "        print(f'{i:2} (score: {score:.5f}): {sentence}')\n",
    "\n",
    "# Perform an example search using BERT\n",
    "print(\"We will search for the question:\", question, \":\\n\")\n",
    "search_bert(question, model, tokenizer, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main theme of 'To Kill a Mockingbird' is the search for self and overcoming prejudices in society.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the OpenAI API key from user input and set it as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "# Initialize a ChatOpenAI object with a specified model\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Define a Document class to structure the document data\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content  # Store the main content of the page\n",
    "        self.metadata = metadata  # Optionally store metadata for the document\n",
    "\n",
    "# Function to load documents from a TSV file and convert them into Document objects\n",
    "def load_tsv_as_docs(tsv_path):\n",
    "    documents = {}\n",
    "    with open(tsv_path, newline='', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # Extract document ID, title, and sentence from the current row\n",
    "            doc_id = row['DocumentID']\n",
    "            title = row['DocumentTitle']\n",
    "            sentence = row['Sentence']\n",
    "            if doc_id not in documents:\n",
    "                documents[doc_id] = {'title': title, 'content': [], 'metadata': {'DocumentID': doc_id, 'DocumentTitle': title}}\n",
    "            documents[doc_id]['content'].append(sentence)\n",
    "    \n",
    "    combined_docs = []\n",
    "    for doc_id, doc_info in documents.items():\n",
    "        doc_content = f\"{doc_info['title']}\\n\" + \"\\n\".join(doc_info['content'])\n",
    "        combined_docs.append(Document(page_content=doc_content, metadata=doc_info['metadata']))\n",
    "    \n",
    "    return combined_docs\n",
    "\n",
    "docs = load_tsv_as_docs('WikiQACorpus/WikiQA.tsv')\n",
    "\n",
    "# Initialize a text splitter to divide documents into smaller chunks for processing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create a vector store from the document splits using specified embeddings\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Convert the vector store into a retriever object\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Pull a pre-defined prompt from a hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define a function to format documents into a string separated by two newlines\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define a retrieval-augmented generation (RAG) chain that combines the retriever, document formatter, prompt, and language model\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}  # Prepare context and question for input\n",
    "    | prompt  # Use the pulled prompt to format the input for the language model\n",
    "    | llm  # Pass the formatted input to the language model for generation\n",
    "    | StrOutputParser()  # Parse the output from the language model into a string\n",
    ")\n",
    "\n",
    "# Invoke the RAG chain with a specific question to get an answer\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document ID: D268\n",
      "Document Title: Pride and prejudice\n",
      "Content:\n",
      "To date, the book has sold some 20 million copies worldwide.\n",
      "As Anna Quindlen wrote, \"Pride and Prejudice is also about that thing that all great novels consider, the search for self.\n",
      "And it is the first great novel to teach us that that search is as surely undertaken in the drawing room making small talk as in the pursuit of a great white whale or the public punishment of adultery .\"\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Similarity Score: 0.40987\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Returns Cosine Distance between the question and the document\n",
    "docs = vectorstore.similarity_search_with_score(query=question)\n",
    "document, score = docs[0]\n",
    "\n",
    "formatted_output = f\"\"\"\n",
    "Document ID: {document.metadata['DocumentID']}\n",
    "Document Title: {document.metadata['DocumentTitle']}\n",
    "Content:\n",
    "{document.page_content}\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Similarity Score: {score:.5f}\n",
    "\"\"\"\n",
    "print(formatted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Direction\n",
    "This project aimed to explore research and methdology in the area of text retrieval and generation models, particularly through the lens of Dense Passage Retrieval (DPR) models, the Pyserini toolkit, and the innovative Retrieval-Augmented Generation (RAG) model. The methodology adopted, spanning from  setup and data preparation to the execution of various retrieval models, provided a comprehensive framework for evaluating and contrasting the capabilities and limitations of each approach in addressing complex user queries. \n",
    "\n",
    "A pivotal aspect of our methodology was the direct comparison of document retrieval models, offering a more robust understanding of their operational differences and effectiveness. Utilizing the Microsoft Research WikiQA Corpus as a testing ground, our findings revealed that while traditional sparse retrieval techniques and the dense retrieval approach offered certain benefits, they also presented limitations in terms of computational demands, contextual understanding, and the freshness of retrieved information. For instance, the Sparse Model using Pyserini showcased an F1-Score of 0.2001, highlighting its limitations in accuracy despite its efficiency in retrieval speed. Conversely, the Dense Model using DPR demonstrated a more nuanced and semantically rich retrieval capability, with an F1-Score of 0.1151, underscoring the improvement over traditional retrieval techniques but also revealing the challenges in balancing precision and recall.\n",
    "\n",
    "The RAG model, with its innovative integration of retrieval and generative components, stood out for its ability to significantly enhance response accuracy and relevance. This model's workflow, from user input through to enhanced response generation, illuminated the potential of combining dense vector retrieval with generative models to produce informative and contextually relevant responses. However, the project also illuminated several limitations inherent in these advanced models. The computational demands of dense retrieval approaches, including DPR, underscore the need for substantial hardware resources, which may not be feasible for all organizations. Additionally, while RAG models mark a significant advancement in retrieval accuracy, they are not immune to generating factually incorrect information, especially when the external knowledge base is outdated or biased. However, it's crucial to note that the RAG model is a relatively new entrant in the field of information retrieval systems. As such, research into its optimal evaluation metrics, implementation strategies, and creation methodologies is still actively ongoing. This burgeoning stage of development means that while the RAG model promises substantial improvements in retrieval accuracy and the quality of generated responses, the academic and professional communities are still exploring the best practices for its deployment and utilization.\n",
    "\n",
    "The exploration into the BERT model for sequence classification further underscored the advanced capabilities of LLMs in text processing and classification. Despite its strengths, the need for meticulous data preparation and the computational resources required for processing large datasets in batches were notable challenges. The methodology's emphasis on practical application, through example searches and the evaluation of retrieval metrics, provided valuable insights into each model's real-world utility. This hands-on approach not only highlighted the models' operational differences but also their potential synergies.\n",
    "\n",
    "Looking forward, the limitations encountered offer a roadmap for future research. The potential for hybrid models that leverage the strengths of both sparse and dense retrieval techniques, alongside generative models, is particularly promising. Further exploration into fine-tuning methodologies for LLMs, tailored for retrieval tasks, could address the factual inaccuracies observed. Additionally, the development of dynamic knowledge bases, updated in real-time, could significantly enhance the RAG model's effectiveness and reliability.\n",
    "In conclusion, the lessons learned from this project, from the practical challenges of implementing advanced models to the theoretical insights gained through comparative analysis, underscore the complexity and potential of this field. As we look to the future, this project lays a foundation for continued innovation and exploration, with the ultimate goal of developing more intelligent, efficient, and accurate retrieval systems that can navigate the vast and ever-expanding digital landscape. The numerical findings, such as the F1-Scores and precision metrics, serve as a quantitative testament to the strengths and weaknesses of each studied model, guiding our understanding of where improvements are most needed and where potential lies for future advancements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- Dai, Zhuyun, and Jamie Callan. “Context-Aware Term Weighting For First Stage Passage Retrieval.” *Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval*, ACM, 2020, pp. 1533–36. [DOI.org (Crossref)](https://doi.org/10.1145/3397271.3401204).\n",
    "- Gao, Yunfan, et al. *Retrieval-Augmented Generation for Large Language Models: A Survey*. arXiv:2312.10997, arXiv, 27 Mar. 2024. [arXiv.org](http://arxiv.org/abs/2312.10997).\n",
    "- Izacard, Gautier, and Edouard Grave. *Distilling Knowledge from Reader to Retriever for Question Answering*. arXiv:2012.04584, arXiv, 4 Aug. 2022. [arXiv.org](http://arxiv.org/abs/2012.04584).\n",
    "- Karpukhin, Vladimir, et al. *Dense Passage Retrieval for Open-Domain Question Answering*. arXiv:2004.04906, arXiv, 30 Sept. 2020. [arXiv.org](http://arxiv.org/abs/2004.04906).\n",
    "- Khodabakhsh, Maryam, and Ebrahim Bagheri. “Learning to Rank and Predict: Multi-Task Learning for Ad Hoc Retrieval and Query Performance Prediction.” *Information Sciences*, vol. 639, Aug. 2023, p. 119015. [DOI.org (Crossref)](https://doi.org/10.1016/j.ins.2023.119015).\n",
    "- Lála, Jakub, et al. *PaperQA: Retrieval-Augmented Generative Agent for Scientific Research*. arXiv:2312.07559, arXiv, 14 Dec. 2023. [arXiv.org](http://arxiv.org/abs/2312.07559).\n",
    "- LangChain. \"Chroma.\" Langchain Documentation, n.p., n.d., https://python.langchain.com/docs/integrations/vectorstores/chroma/. Accessed 19 April. 2024.\n",
    "- LangChain. \"Q&A with RAG.\" Langchain Documentation, n.p., n.d., https://python.langchain.com/docs/get_started/introduction#guides. Accessed 19 April. 2024.\n",
    "- Lewis, Patrick, et al. *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*. arXiv:2005.11401, arXiv, 12 Apr. 2021. [arXiv.org](http://arxiv.org/abs/2005.11401).\n",
    "- Luan, Yi, et al. *Sparse, Dense, and Attentional Representations for Text Retrieval*. arXiv:2005.00181, arXiv, 16 Feb. 2021. [arXiv.org](http://arxiv.org/abs/2005.00181).\n",
    "- Penha, Gustavo, and Claudia Hauff. *Sparse and Dense Approaches for the Full-Rank Retrieval of Responses for Dialogues*. arXiv:2204.10558, arXiv, 22 Apr. 2022. [arXiv.org](http://arxiv.org/abs/2204.10558).\n",
    "- Sun, Weiwei, et al. *Learning to Tokenize for Generative Retrieval*. arXiv:2304.04171, arXiv, 9 Apr. 2023. [arXiv.org](http://arxiv.org/abs/2304.04171).\n",
    "- Yang, Yi, Wen-tau Yih, and Christopher Meek. \"WikiQA: A Challenge Dataset for Open-Domain Question Answering.\" *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)*. Association for Computational Linguistics, September 2015, Lisbon, Portugal.\n",
    "- Zhan, Jingtao, et al. *Learning To Retrieve: How to Train a Dense Retrieval Model Effectively and Efficiently*. arXiv:2010.10469, arXiv, 20 Oct. 2020. [arXiv.org](http://arxiv.org/abs/2010.10469)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
